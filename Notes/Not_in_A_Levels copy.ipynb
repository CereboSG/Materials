{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Functional Programming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\r\n",
    "\r\n",
    "1. Lane, Hobson, et al. *Natural Language Processing in Action: Understanding, Analyzing, and Generating Text with Python*. Manning Publications, 2019. \r\n",
    "2. https://moj-analytical-services.github.io/NLP-guidance/Glossary.html\r\n",
    "3. https://math.stackexchange.com/questions/320220/intuitively-what-is-the-difference-between-eigendecomposition-and-singular-valu"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Natural Language Processing (NLP)\r\n",
    "\r\n",
    "**Tokenization** is the *process of breaking up a phrase, sentence, paragraph, or an entire text document into smaller unit*s, such as individual words or terms. Each of these smaller units are called **tokens**.\r\n",
    "\r\n",
    "An **$n$-gram** is a *subsequence containing up to $n$ elements from a sequence of elements*, usually a string. in particular, an $n$-gram of words are just subsequence of a sequence of words.\r\n",
    "\r\n",
    "A **document** is a text object, the collection of which make up your corpus. If you are doing work on Search or Topics, the documents will be the objects which you will be finding similarities between in order to group them topically. The length and definition of a document will depend on the question you are answering.\r\n",
    "\r\n",
    "A **corpus** is the set of text documents you are analysing.\r\n",
    "\r\n",
    "A **vocabulary** is the set of all words used in the corpus, after stopwords have been removed and stemming has been done (where appropriate).\r\n",
    "\r\n",
    "Let $t$ be a given term in a given document $d$ found in a corpus $D$. We define the following:\r\n",
    "- **term frequency (TF)** $\\operatorname{tf}(t,d) $ is the ratio of the occurence of $t$ in the document $d$, i.e., $$\\operatorname{tf}\\left(t,d\\right)= \\frac{\\operatorname{count}(t)}{\\operatorname{count}(d)}$$\r\n",
    "- **inverse document frequency (IDF)** $\\operatorname{idf}(t,D)$ is the logarithm of the ratio of number of documents in the corpus to the number of documents containing $t$, i.e., $$\\operatorname{idf}\\left(t,D\\right)= \\log\\frac{\\mid D\\mid}{|\\{ d\\in D : t\\in d \\}|}$$\r\n",
    "- **TF-IDF** $\\operatorname{tfidf}(t,d,D) = \\operatorname{tf}(t,d)\\cdot \\operatorname{idf}(t,D) $\r\n",
    "\r\n",
    "An **embedding** is the process whereby documents or words are coded up as a vector in some (typically very high-dimensional) vector space, e.g.,\r\n",
    "- document vector of word count\r\n",
    "- document vector of TF-IDF value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example\r\n",
    "- `scikit-learn` has `CountVectorizer` class that gives document vectors with word count.\r\n",
    "- `scikit-learn` module alrady the `TfidVectorizer` class that uses `fit_transform()` method to give us a TF-IDF matrix given a corpus $D$.\r\n",
    "- `get_feature_names()` method will give the terms in the TF-IDF matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Hary would get home.\"]\r\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\r\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\r\n",
    "\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "\r\n",
    "corpus = docs\r\n",
    "vectorizer = CountVectorizer(min_df=1)\r\n",
    "model = vectorizer.fit_transform(corpus)\r\n",
    "print(model.todense().round(2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 0 3 1 1 0 1 1 1 0 0 0 1 0 3 1 1]\n",
      " [1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0]\n",
      " [0 2 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "vectorizer.get_feature_names()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'hary',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `casual_tokenize` function exists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\r\n",
    "\r\n",
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\r\n",
    "\r\n",
    "tokenizer = TreebankWordTokenizer()\r\n",
    "tokenizer.tokenize(sentence)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from nltk.util import ngrams\r\n",
    "\r\n",
    "print(list(ngrams(tokenizer.tokenize(sentence),2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Monticello', 'was'), ('was', \"n't\"), (\"n't\", 'designated'), ('designated', 'as'), ('as', 'UNESCO'), ('UNESCO', 'World'), ('World', 'Heritage'), ('Heritage', 'Site'), ('Site', 'until'), ('until', '1987'), ('1987', '.')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Latent Semantic Analysis (LSA) or Singular Value decomposition (SVD)\r\n",
    "\r\n",
    "In the context of natural language processing, LSA or SVD works as follows: Let $W_{m\\times n}$ be a $m\\times n$ be a **term-document matrix** (could be BOW or TF-IDF or whatever) with $m$ terms and $n$ documents in your corpus. By SVD, we mean that we are rewriting $W_{m\\times n}$ as a product of matrices $U_{m\\times p}$, $S_{p\\times p}$ and $V_{p\\times n}$, i.e. $$ W_{m\\times n} =  U_{m\\times p} S_{p\\times p} V_{p\\times n},$$ where $p$ is the number of topics in your corpus.\r\n",
    "\r\n",
    "- $U_{m\\times p}$ is called the **term-topic matrix** , which measures the cross-correlation between terms and topics based on word co-occurence in the same document, i.e the value in each cell is a weight that represents how important each word is to each topic.\r\n",
    "    - Note that topic vectors are automatically generated by SVD.\r\n",
    "- $S_{p\\times p}$ is called the **topic-topic matrix**, whose nondiagonal entries are zero and each entry in the diagonal indicates the importance of each topic.\r\n",
    "    - `svd()` method in `numpy` already arrange $S_{p\\times p}$ sch that the largest singular values in on the left.\r\n",
    "\r\n",
    "- $V_{p\\times n}$ is called the **document-document matrix**, and it measures how often documents use the same topics in your new semantic model of the documents.\r\n",
    "\r\n",
    "## Example\r\n",
    "\r\n",
    "Let `docs` be your corpus.\r\n",
    "\r\n",
    ">```\r\n",
    "> docs = [\"The faster Harry got to the store, the faster and faster Hary would get home.\",\r\n",
    ">         \"Harry is hairy and faster than Jill.\",\r\n",
    ">         \"Jill is not as hairy as Harry.\"]\r\n",
    ">```\r\n",
    "\r\n",
    "Then the document-term matrix (BOW) of `docs` is \r\n",
    "\r\n",
    "|         | and | as | faster | get | got | hairy | harry | hary | home | is | jill | not | store | than | the | to | would |\r\n",
    "|---------|-----|----|--------|-----|-----|-------|-------|------|------|----|------|-----|-------|------|-----|----|-------|\r\n",
    "| docs[0] | 1   | 0  | 3      | 1   | 1   | 0     | 1     | 1    | 1    | 0  | 0    | 0   | 1     | 0    | 3   | 1  | 1     |\r\n",
    "| docs[1] | 1   | 0  | 1      | 0   | 0   | 1     | 1     | 0    | 0    | 1  | 1    | 0   | 0     | 1    | 0   | 0  | 0     |\r\n",
    "| docs[2] | 0   | 2  | 0      | 0   | 0   | 0     | 1     | 1    | 0    | 0  | 1    | 1   | 1     | 0    | 0   | 0  | 0     |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Term-document in BOW model\r\n",
    "#We could also use TF-IDF model, just change CountVectorizer to TfidfVectorizer\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "\r\n",
    "docs = [\"The faster Harry got to the store, the faster and faster Hary would get home.\"]\r\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\r\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\r\n",
    "\r\n",
    "corpus = docs\r\n",
    "\r\n",
    "#min_df = k is a parameter that tells the CountVectorizer object to ignore terms that appear in less than k document. \r\n",
    "vectorizer = CountVectorizer(min_df=1)\r\n",
    "\r\n",
    "#fit_transform() is the method that accepts a corpus and returns the term-document matrix\r\n",
    "model = vectorizer.fit_transform(corpus)\r\n",
    "\r\n",
    "#get the \r\n",
    "vectorizer.get_feature_names()\r\n",
    "\r\n",
    "print(model.todense().round(2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monoid and Monads\n",
    "\n",
    "https://andrewdblevins.github.io/Monoids/\n",
    "\n",
    "In programming, functions form a monoid under composition.\n",
    "\n",
    "http://www.philipzucker.com/computational-category-theory-in-python-i-dictionaries-for-finset/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# $\\lambda$-calculus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "import itertools\r\n",
    "\r\n",
    "def pow_set(X:frozenset):\r\n",
    "    if X==frozenset():\r\n",
    "        return frozenset()\r\n",
    "    if len(frozenset(X))==1:\r\n",
    "        return {frozenset(),frozenset(X)}\r\n",
    "    else:\r\n",
    "        comb = combinations(X, len(X)-1)\r\n",
    "        comb = [pow_set(i) for i in comb]\r\n",
    "        return frozenset(X).union(*comb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "X={5}\r\n",
    "\r\n",
    "pow_set(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{frozenset(), frozenset({5})}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "from itertools import combinations\r\n",
    "  \r\n",
    "# Get all combinations of [1, 2, 3]\r\n",
    "# and length 2\r\n",
    "comb = combinations([1, 2, 3], 2)\r\n",
    "  \r\n",
    "# Print the obtained combinations\r\n",
    "print(comb)\r\n",
    "comb=[frozenset(i) for i in comb]\r\n",
    "print(list(comb))\r\n",
    "frozenset().union(*comb)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<itertools.combinations object at 0x0000029B8BD79098>\n",
      "[frozenset({1, 2}), frozenset({1, 3}), frozenset({2, 3})]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "frozenset({1, 2, 3})"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "X=frozenset({1,2,3})\r\n",
    "print(pow_set(X))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "frozenset({1, 2, 3, frozenset({2}), frozenset({3}), frozenset({1}), frozenset()})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "comb = combinations(frozenset({1, 2, 3}), 2)\r\n",
    "print(list(comb))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(1, 2), (1, 3), (2, 3)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "len({1,2,3})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "- Term is invented by Richard Bellman to hide the fact that he was doing a mathematical research (NOT REALLY PROGRAMMING!!!)\n",
    "- Memoized\n",
    "\n",
    "## Reference:\n",
    "- Dynamic Programming I: Fibonacci, Shortest Paths - Erik Demaine https://www.youtube.com/watch?v=OQ5jsbhAv_M \n",
    "\n",
    "## "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit"
  },
  "interpreter": {
   "hash": "9855bcb2db2fc1adb021fa4ac9c7dfbb81452ef889b3ef5659154f2722c9fc88"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}